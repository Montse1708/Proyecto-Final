























# Importamos módulos para la carga y manejo de datos
import pyspark.sql.functions as f 
import pyspark.sql.types as t


# Creamos sesión de Spark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("IT_Salary") \
    .getOrCreate()


# Cargamos los datos de 2018
datasets_path = 'data/'
dataset2018_path=f'{datasets_path}/IT Salary Survey EU  2018.csv'
salaries2018 = spark.read.format('csv').option('header', True).load(datasets_path)
salaries2018.show(truncate=False, vertical = True)


# Cargamos los datos de 2019
dataset2019_path=f'{datasets_path}/IT Salary Survey EU  2019.csv'
salaries2019 = spark.read.format('csv').option('header', True).load(datasets_path)
salaries2019.show(truncate=False, vertical = True)


# Cargamos los datos de 2020
datasets_path = 'data/'
dataset2020_path=f'{datasets_path}/IT Salary Survey EU  2020.csv'
salaries2020 = spark.read.format('csv').option('header', True).load(datasets_path)
salaries2020.show(truncate=False, vertical = True)














### Verificamos estructura de los salarios
salaries2020.printSchema()




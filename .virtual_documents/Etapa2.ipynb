




















# Importamos módulos para la carga y manejo de datos
import pyspark.sql.functions as f 
import pyspark.sql.types as t


# Creamos sesión de Spark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Etapa 2") \
    .getOrCreate()


# Cargamos los datos de 2018
datasets_path = 'data/'
dataset2018_path=f'{datasets_path}/IT Salary Survey EU  2018.csv'
salaries2018 = spark.read.format('csv').option('header', True).load(datasets_path)
#salaries2018.show(truncate=False)


# Cargamos los datos de 2019
dataset2019_path=f'{datasets_path}/IT Salary Survey EU  2019.csv'
salaries2019 = spark.read.format('csv').option('header', True).load(datasets_path)
#salaries2019.show(truncate=False)


# Cargamos los datos de 2020
datasets_path = 'data/'
dataset2020_path=f'{datasets_path}/IT Salary Survey EU  2020.csv'
salaries2020 = spark.read.format('csv').option('header', True).load(datasets_path)
#salaries2020.show(truncate=False)



